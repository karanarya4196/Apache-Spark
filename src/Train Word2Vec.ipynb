{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import (SparkSession, functions as F, types as T)\n",
    "spark = (SparkSession.builder.master('yarn')\n",
    "         .appName(\"LBA\")\n",
    "         .config('spark.master', 'local[*]')\n",
    "         .config('spark.sql.cbo.enabled', True)\n",
    "         .config('spark.sql.cbo.joinReorder.enabled', True)\n",
    "         .config(\"spark.executor.instances\",10)\n",
    "         .config(\"spark.executor.cores\",5)\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config('spark.driver.memory', '4g')\n",
    "         .config('spark.yarn.executor.memoryOverhead', '2g')\n",
    "         .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.2.3\")\n",
    "         .config(\"spark.port.maxRetries\", 100)\n",
    "         .config('spark.default.parallelism', 4)\n",
    "         .config('spark.kryoserializer.buffer.max', '512m')\n",
    "         .getOrCreate()\n",
    "         )\n",
    "from pyspark.ml.feature import (Tokenizer, NGram, RegexTokenizer as smlRegexTokenizer,\n",
    "                                StopWordsRemover as sml_StopWordsRemover,CountVectorizer, \n",
    "                                CountVectorizerModel, IDF,IDFModel, Word2Vec, StringIndexer, \n",
    "                                VectorIndexer, IndexToString, QuantileDiscretizer, VectorAssembler\n",
    "                               )\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.classification import (RandomForestClassifier,\n",
    "                                       RandomForestClassificationModel)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                  MulticlassClassificationEvaluator)\n",
    "from pyspark.ml.tuning import (CrossValidator, CrossValidatorModel,\n",
    "                               ParamGridBuilder)\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector, Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "pd.set_option('display.max_columns', 999)\n",
    "import logging\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from itertools import combinations as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of dataframe is:', 5416067, 51)\n",
      "('Columns of dataframe are:', ['_c0', 'Louanna O. Heuhsen', '_c2', '2019-01-03 00:00:00', '2018-12-31 00:00:004', '591572499', 'L.HEUHSENFEES2', '2018-12-03 00:00:00', 'Paid', '2019-04-01 17:02:45.513', '30363724', '_c11', '_c12', '140000.0', '0.014', '2018-12-31 00:00:0015', '$35K/week (4 weeks)', '0.017', '1.0', '0.019', '_c20', '0.021', '0.022', '0.023', '0.024', '0.025', '14735498.0', '81621.0', '2019-01-03 17:51:01.277', '_c29', '(Unspecified)', 'A990', 'Summary and Phase Invoice Entry Template Fees', 'Fee33', 'Fee34', 'Fee35', 'Fee36', 'Strategy & Business Development', 'Altria', '_c39', '_c40', '_c41', '_c42', '_c43', '_c44', '_c45', '_c46', '_c47', '_c48', '_c49', 'left_only'])\n"
     ]
    }
   ],
   "source": [
    "def readData(filePath):\n",
    "    df = spark.read.option('inferSchema', 'True').option('header', 'True').csv(filePath)\n",
    "    print('Shape of dataframe is:', df.count(), len(df.columns))\n",
    "    print('Columns of dataframe are:', df.columns)\n",
    "    return df\n",
    "\n",
    "df = readData('word2vec_df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5416067"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(F.col('$35K/week (4 weeks)').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5100260"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDescription(df):\n",
    "\n",
    "    data = df[['$35K/week (4 weeks)']]\n",
    "    \n",
    "    documentAssembler = (DocumentAssembler() \n",
    "      .setInputCol('$35K/week (4 weeks)') \n",
    "      .setOutputCol(\"document\") \n",
    "      )\n",
    "\n",
    "    tokenizer = (RegexTokenizer() \n",
    "    .setInputCols([\"document\"])\n",
    "    .setOutputCol(\"token\") \n",
    "    .setPattern(\"\\w+\")\n",
    "    )\n",
    "\n",
    "\n",
    "    normalizer = (Normalizer() \n",
    "      .setInputCols([\"token\"]) \n",
    "      .setOutputCol(\"normalized\")\n",
    "      .setPattern(\"[^A-Za-z]\")\n",
    "      )\n",
    "\n",
    "    stemmer = (Stemmer() \n",
    "       .setInputCols([\"normalized\"]) \n",
    "       .setOutputCol(\"stem\")\n",
    "       )\n",
    "\n",
    "    \n",
    "    finisher = (Finisher() \n",
    "    .setInputCols([\"stem\"])\n",
    "    .setOutputCols([\"stem_tokens\"])\n",
    "    .setAnnotationSplitSymbol(', ')\n",
    "    .setValueSplitSymbol('|')\n",
    "    .setCleanAnnotations(True)\n",
    "    .setIncludeKeys(False)\n",
    "    .setOutputAsArray(True)\n",
    "    )\n",
    "\n",
    "    remover = sml_StopWordsRemover(inputCol='stem_tokens', \n",
    "                              outputCol=\"clean_reviews\",\n",
    "                              stopWords=stopwordList)\n",
    "    \n",
    "    nlp_pipeline = Pipeline() \\\n",
    "      .setStages([\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        normalizer, \n",
    "        stemmer,\n",
    "        finisher,\n",
    "        remover\n",
    "      ])\n",
    "\n",
    "    nlp_pl_stages = list(enumerate(nlp_pipeline.getStages()))\n",
    "    nlp_pl_model = nlp_pipeline.fit(data)\n",
    "    clean_description = (nlp_pl_model.transform(data))\n",
    "    return clean_description, df\n",
    "\n",
    "clean_description, df = cleanDescription(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5100260"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_description.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$35K/week (4 weeks)', 'stem_tokens', 'clean_reviews']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|         word|              vector|\n",
      "+-------------+--------------------+\n",
      "|        frane|[0.07755979895591...|\n",
      "|ratingschedul|[0.04851161688566...|\n",
      "|        shuvo|[-0.0091098425909...|\n",
      "|       hilyer|[0.02221369184553...|\n",
      "|      rahmani|[-0.0260612219572...|\n",
      "|        epoca|[0.08312328159809...|\n",
      "| positionstat|[-0.0698190852999...|\n",
      "|chargingparti|[0.03715729713439...|\n",
      "|        nejim|[0.00212544575333...|\n",
      "|         stum|[-0.0501778051257...|\n",
      "|         ihss|[-0.0148747991770...|\n",
      "|        cerio|[0.11717541515827...|\n",
      "|     zasypkin|[-0.0603234581649...|\n",
      "|    attorneya|[-0.1003433242440...|\n",
      "|     mattsson|[-0.1063998863101...|\n",
      "|   hirschhorn|[0.05476534366607...|\n",
      "|       dimsis|[0.01609681546688...|\n",
      "|     clarissa|[0.26560556888580...|\n",
      "|    bourgouin|[-0.0607305541634...|\n",
      "|    dellacqua|[0.12180265039205...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec = (Word2Vec()\n",
    "                .setInputCol(\"clean_reviews\")\n",
    "                .setOutputCol(\"W2V\")\n",
    "                .setMinCount(1)\n",
    "                .setNumPartitions(1)\n",
    "                .setStepSize(0.1)\n",
    "                .setWindowSize(10)\n",
    "                .setVectorSize(400)\n",
    "                .setMaxSentenceLength(20)\n",
    "              )  \n",
    "model = word2Vec.fit(clean_description)\n",
    "\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dict = model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dict.write.parquet('w2v_dict.parquet', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('filepath_to_just_word2vec_not_its_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec.save('word2vecoriginal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = Word2Vec.load('word2vecoriginal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded.getVectorSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modeloriginal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedmodel = Word2VecModel.load('modeloriginal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|         word|              vector|\n",
      "+-------------+--------------------+\n",
      "|        frane|[0.07755979895591...|\n",
      "|ratingschedul|[0.04851161688566...|\n",
      "|        shuvo|[-0.0091098425909...|\n",
      "|       hilyer|[0.02221369184553...|\n",
      "|      rahmani|[-0.0260612219572...|\n",
      "|        epoca|[0.08312328159809...|\n",
      "| positionstat|[-0.0698190852999...|\n",
      "|chargingparti|[0.03715729713439...|\n",
      "|        nejim|[0.00212544575333...|\n",
      "|         stum|[-0.0501778051257...|\n",
      "|         ihss|[-0.0148747991770...|\n",
      "|        cerio|[0.11717541515827...|\n",
      "|     zasypkin|[-0.0603234581649...|\n",
      "|    attorneya|[-0.1003433242440...|\n",
      "|     mattsson|[-0.1063998863101...|\n",
      "|   hirschhorn|[0.05476534366607...|\n",
      "|       dimsis|[0.01609681546688...|\n",
      "|     clarissa|[0.26560556888580...|\n",
      "|    bourgouin|[-0.0607305541634...|\n",
      "|    dellacqua|[0.12180265039205...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadedmodel.getVectors().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
